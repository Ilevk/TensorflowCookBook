{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "01_Implement_RNN_for_SPam_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 250\n",
    "max_sequence_length = 25\n",
    "rnn_size = 10\n",
    "embedding_size = 50\n",
    "min_word_frequency = 10\n",
    "learning_rate = 0.0005\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Spam Data\n",
    "data_dir = 'temp'\n",
    "data_file = 'text_data.txt'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii', errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    \n",
    "    with open(os.path.join(data_dir,data_file), 'w') as file_conn:\n",
    "        for text in text_data:\n",
    "            file_conn.write(\"{}\\n\".format(text))\n",
    "else:\n",
    "    text_data = []\n",
    "    with open(os.path.join(data_dir,data_file), 'r') as file_conn:\n",
    "        for row in file_conn:\n",
    "            text_data.append(row)\n",
    "    text_data = text_data[:-1]\n",
    "\n",
    "text_data = [x.split('\\t') for x in text_data if len(x) >=1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+','',text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return text_string\n",
    "\n",
    "text_data_train = [clean_text(x) for x in text_data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get IndexList about vocab Data\n",
    "vocab_processor = tf.contrib.learn.preprocessing.\\\n",
    "    VocabularyProcessor(max_sequence_length,min_frequency=min_word_frequency)\n",
    "\n",
    "text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling Data\n",
    "text_processed = np.array(text_processed)\n",
    "text_data_target = np.array([1 if x=='ham' else 0 for x in text_data_target])\n",
    "#Get random index\n",
    "shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\n",
    "#Shuffing\n",
    "x_shuffled = text_processed[shuffled_ix]\n",
    "y_shuffled = text_data_target[shuffled_ix]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 933\n80-20 Train Test split: 4459 == 1115\n"
     ]
    }
   ],
   "source": [
    "#Split Data to Train 80% , Test 20%\n",
    "#If we handle hyperparameter, shoule be splitting Train - Test - Valid data\n",
    "ix_cutoff = int(len(y_shuffled) * 0.80)\n",
    "x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "print(\"80-20 Train Test split: {:d} == {:d}\".format(len(y_train),len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define placeholder\n",
    "x_data = tf.placeholder(tf.int32,[None, max_sequence_length])\n",
    "y_output = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embedding matrix for finding embeding value in x\n",
    "embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0,1.0))\n",
    "#operatin to find embedding\n",
    "embedding_output = tf.nn.embedding_lookup(embedding_mat,x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dynamic RNN model\n",
    "#dynamic RNN can handle data that have different length each\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size, reuse=tf.AUTO_REUSE)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
    "output = tf.nn.dropout(output, dropout_keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearange RNN result\n",
    "output = tf.transpose(output, [1,0,2])\n",
    "#end to get prediction output, split last output value\n",
    "last = tf.gather(output, int(output.get_shape()[0]) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to finialize prediction transpose output value of rnn_size to logits_output through \n",
    "#fully connected layer\n",
    "weight = tf.Variable(tf.truncated_normal([rnn_size, 2],\n",
    "                                         stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "logits_out = tf.matmul(last, weight) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss func\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out,\n",
    "                                                        labels=y_output)\n",
    "#logits = float32, labels=int32\n",
    "loss = tf.reduce_mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for comparison with train set, test set's algorithm performance\n",
    "#define accuracy func Also \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out,1), \n",
    "                                           tf.cast(y_output,tf.int64)),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#RMS method to minimize loss func\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Test Loss: 0.66, Test Acc: 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, Test Loss: 0.62, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, Test Loss: 0.57, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, Test Loss: 0.52, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, Test Loss: 0.47, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, Test Loss: 0.44, Test Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, Test Loss: 0.42, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, Test Loss: 0.41, Test Acc: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, Test Loss: 0.4, Test Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, Test Loss: 0.4, Test Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11, Test Loss: 0.39, Test Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12, Test Loss: 0.39, Test Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13, Test Loss: 0.39, Test Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14, Test Loss: 0.39, Test Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15, Test Loss: 0.38, Test Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16, Test Loss: 0.38, Test Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17, Test Loss: 0.38, Test Acc: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18, Test Loss: 0.38, Test Acc: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19, Test Loss: 0.37, Test Acc: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, Test Loss: 0.37, Test Acc: 0.88\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "#Training\n",
    "for epoch in range(epochs):\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "    x_train = x_train[shuffled_ix]\n",
    "    y_train = y_train[shuffled_ix]\n",
    "    num_batches = int(len(x_train) / batch_size) + 1\n",
    "    for i in range(num_batches):\n",
    "        min_ix = i * batch_size\n",
    "        max_ix = np.min([len(x_train), ((i + 1) * batch_size)])\n",
    "        x_train_batch = x_train[min_ix:max_ix]\n",
    "        y_train_batch = y_train[min_ix:max_ix]\n",
    "\n",
    "        train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob: 0.5}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_acc = sess.run([loss, accuracy],\n",
    "                                               feed_dict=train_dict)\n",
    "    train_loss.append(temp_train_loss)\n",
    "    train_accuracy.append(temp_train_acc)\n",
    "    test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob: 1.0}\n",
    "    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n",
    "    test_loss.append(temp_test_loss)\n",
    "    test_accuracy.append(temp_test_acc)\n",
    "    print('Epoch : {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "epoch_seq = np.arange(1, epochs+1)\n",
    "plt.plot(epoch_seq, train_loss, 'k--', label='Train Set')\n",
    "plt.plot(epoch_seq, test_loss, 'r-', label ='Test Set')\n",
    "plt.title('Softmax Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sofrmax Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_seq, train_accuracy,'k--', label='Train Set')\n",
    "plt.plot(epoch_seq, test_accuracy,'r-', label='Test Set')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
